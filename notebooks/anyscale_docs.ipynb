{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anyscale docs | RAG-based LLM chatbot\n",
    "\n",
    "This notebook builds a local chatbot that answers questions about Anyscale and Ray by pulling relevant context from docs.anyscale.com and docs.ray.io. All documents are embedded with Open AI's `text-embedding-3-large` and queries route through `gpt-4o`.\n",
    "\n",
    "* **Organization:** anyscale-internal (staging)\n",
    "* **Cloud:** `anyscale_v2_default_cloud`\n",
    "* **Region:** AWS `us-west-2`\n",
    "* **Compute config:** `g3.8xlarge` head node, which has 2 GPUs and 32 CPUs\n",
    "* **Base image:** `anyscale/ray:2.35.0-py312`\n",
    "* **GitHub repository (forked from ray-project/llm-applications):** https://github.com/emmyscode/anyscale-ragbot\n",
    "* **Workspace:** https://console.anyscale-staging.com/v2/cld_kvedZWag2qA8i5BjxUevf5i7/prj_xQEUDtTyHnTLLx77QG9jRvWY/workspaces/expwrk_9n7tvbjpl7zwhwu33s7r683phe/ses_vig1su7dbnvzqbs3l1tw35fhk3?workspace-tab=code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Initialize the environment\n",
    "\n",
    "Once you have the workspace up, complete the following:  \n",
    "\n",
    "1. Clone the repository\n",
    "    - `git clone https://github.com/emmyscode/anyscale-ragbot.git` \n",
    "2. Install dependencies\n",
    "    - `pip install -r requirements.txt`\n",
    "    - `export PYTHONPATH=$PYTHONPATH:$PWD`\n",
    "    - `pre-commit install`\n",
    "    - `pre-commit autoupdate`\n",
    "3. Set up credentials\n",
    "    - `touch .env`\n",
    "    - Add in `OPENAI_API_BASE`, `OPENAI_API_KEY`, and `DB_CONNECTION_STRING` to the `.env` file\n",
    "    - `source .env`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ray\n",
    "import sys; sys.path.append(\"..\")\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from dotenv import load_dotenv; load_dotenv()\n",
    "from rag.config import EMBEDDING_DIMENSIONS, MAX_CONTEXT_LENGTHS, ROOT_DIR\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the Ray cluster, with relevant credentials.\n",
    "\n",
    "ray.init(runtime_env={\n",
    "    \"env_vars\": {\n",
    "        \"OPENAI_API_BASE\": os.environ[\"OPENAI_API_BASE\"],\n",
    "        \"OPENAI_API_KEY\": os.environ[\"OPENAI_API_KEY\"], \n",
    "        \"DB_CONNECTION_STRING\": os.environ[\"DB_CONNECTION_STRING\"],\n",
    "    },\n",
    "    \"working_dir\": str(ROOT_DIR)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "I've pre-loaded the data into `/mnt/shared_storage/emmy` for both Ray docs (`/mnt/shared_storage/emmy/docs.ray.io/en/master`) and Anyscale docs (`/mnt/shared_storage/emmy/docs.anyscale.com/docs`) respectively. So in this section, we'll clean and chunk the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from rag.config import EFS_DIR\n",
    "\n",
    "ANYSCALE_DOCS_DIR = Path(EFS_DIR, \"docs.anyscale.com/docs\")\n",
    "ANYSCALE_DOCS_URL = \"https://docs.anyscale.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This part creates the Anyscale sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of dictionaries, each containing the source and text\n",
    "data = []\n",
    "for path in ANYSCALE_DOCS_DIR.rglob(\"*.md\"):\n",
    "    if not path.is_dir():\n",
    "        with open(path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        # Convert the file path to a URL, remove the '.md' extension\n",
    "        relative_path = path.relative_to(ANYSCALE_DOCS_DIR).with_suffix('')  # Remove the '.md'\n",
    "        source = f\"{ANYSCALE_DOCS_URL}/{relative_path.as_posix()}\"\n",
    "        data.append({\"source\": source, \"text\": text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anyscale_sections_ds = ray.data.from_items(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This part creates the Ray sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag.data import extract_sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAY_DOCS_DIR = Path(EFS_DIR, \"docs.ray.io/en/master\")\n",
    "ray_ds = ray.data.from_items([{\"path\": path} for path in RAY_DOCS_DIR.rglob(\"*.html\") if not path.is_dir()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray_sections_ds = ray_ds.flat_map(extract_sections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anyscale md chunking first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_md(md_doc):\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "    ]\n",
    "\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=headers_to_split_on, \n",
    "        strip_headers=False\n",
    "        )\n",
    "    \n",
    "    chunks = markdown_splitter.split_text(md_doc[\"text\"])\n",
    "    return[{\"text\": chunk.page_content, \"source\": md_doc[\"source\"]} for chunk in chunks]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_ds = anyscale_sections_ds.flat_map(chunk_md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then Ray chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_section(section, chunk_size, chunk_overlap):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len)\n",
    "    chunks = text_splitter.create_documents(\n",
    "        texts=[section[\"text\"]], \n",
    "        metadatas=[{\"source\": section[\"source\"]}])\n",
    "    return [{\"text\": chunk.page_content, \"source\": chunk.metadata[\"source\"]} for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray_chunks_ds = ray_sections_ds.flat_map(partial(\n",
    "    chunk_section,\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed\n",
    "\n",
    "To simplify, I'm just doing OpenAI across the board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anyscale first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = OpenAIEmbeddings(\n",
    "            model=\"text-embedding-3-large\",\n",
    "            openai_api_base=os.environ[\"OPENAI_API_BASE\"],\n",
    "            openai_api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbedChunks:\n",
    "    def __init__(self):\n",
    "        self.embedding_model = OpenAIEmbeddings(\n",
    "            model=\"text-embedding-3-large\",\n",
    "            openai_api_base=os.environ[\"OPENAI_API_BASE\"],\n",
    "            openai_api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    "            )\n",
    "    def __call__(self, batch):\n",
    "        embeddings = self.embedding_model.embed_documents(batch[\"text\"])\n",
    "        return {\"text\": batch[\"text\"], \"source\": batch[\"source\"], \"embeddings\": embeddings}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_chunks = chunks_ds.map_batches(\n",
    "    EmbedChunks,\n",
    "    batch_size=100, \n",
    "    num_gpus=1,\n",
    "    concurrency=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ray next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray_embedded_chunks = ray_chunks_ds.map_batches(\n",
    "    EmbedChunks,\n",
    "    batch_size=100, \n",
    "    num_gpus=1,\n",
    "    concurrency=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg\n",
    "from pgvector.psycopg import register_vector\n",
    "\n",
    "embedding_model_name = \"text-embedding-3-large\"\n",
    "\n",
    "os.environ[\"MIGRATION_FP\"] = f\"../migrations/vector-{EMBEDDING_DIMENSIONS[embedding_model_name]}.sql\"\n",
    "os.environ[\"SQL_DUMP_FP\"] = f\"{EFS_DIR}/sql_dumps/{embedding_model_name.split('/')[-1]}.sql\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Set up\n",
    "psql \"$DB_CONNECTION_STRING\" -c \"DROP TABLE IF EXISTS document;\"\n",
    "echo $MIGRATION_FP\n",
    "sudo -u postgres psql -f $MIGRATION_FP\n",
    "echo $SQL_DUMP_FP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Drop the existing `document` table and create a new one with the schema to store embeddings. \n",
    "psql \"$DB_CONNECTION_STRING\" -c \"DROP TABLE IF EXISTS document;\"  # drop\n",
    "sudo -u postgres psql -f $MIGRATION_FP  # create\n",
    "psql \"$DB_CONNECTION_STRING\" -c \"SELECT count(*) FROM document;\"  # num rows\n",
    "\n",
    "# DROP TABLE\n",
    "# CREATE TABLE\n",
    "#  count \n",
    "# -------\n",
    "#      0\n",
    "# (1 row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoreResults:\n",
    "    def __call__(self, batch):\n",
    "        with psycopg.connect(os.environ[\"DB_CONNECTION_STRING\"]) as conn:\n",
    "            register_vector(conn)\n",
    "            with conn.cursor() as cur:\n",
    "                for text, source, embedding in zip(batch[\"text\"], batch[\"source\"], batch[\"embeddings\"]):\n",
    "                    cur.execute(\"INSERT INTO document (text, source, embedding) VALUES (%s, %s, %s)\", (text, source, embedding,),)\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index Anyscale docs\n",
    "embedded_chunks.map_batches(\n",
    "    StoreResults,\n",
    "    batch_size=128,\n",
    "    num_cpus=1,\n",
    "    concurrency=6,\n",
    ").materialize()\n",
    "\n",
    "# Verify whether the embedding was stored successfully in Postgres or not.\n",
    "# sudo -u postgres psql\n",
    "# SELECT * FROM document LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index Ray docs\n",
    "ray_embedded_chunks.map_batches(\n",
    "    StoreResults,\n",
    "    batch_size=128,\n",
    "    num_cpus=1,\n",
    "    concurrency=6,\n",
    ").materialize()\n",
    "\n",
    "# Verify whether the embedding was stored successfully in Postgres or not.\n",
    "# sudo -u postgres psql\n",
    "# SELECT * FROM document LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Save index\n",
    "rm -rf $SQL_DUMP_FP\n",
    "mkdir -p $(dirname \"$SQL_DUMP_FP\") && touch $SQL_DUMP_FP\n",
    "sudo -u postgres pg_dump -c > $SQL_DUMP_FP  # save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed query\n",
    "embedding_model = OpenAIEmbeddings(model=embedding_model_name)\n",
    "query = \"What are the different kinds of storage, and how do I use them?\"\n",
    "embedding = np.array(embedding_model.embed_query(query))\n",
    "len(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get context\n",
    "num_chunks = 10\n",
    "with psycopg.connect(os.environ[\"DB_CONNECTION_STRING\"]) as conn:\n",
    "    register_vector(conn)\n",
    "    with conn.cursor() as cur:\n",
    "        # cur.execute(\"SELECT * FROM document ORDER BY embedding <=> %s LIMIT %s\", (embedding, num_chunks))\n",
    "        cur.execute(\"SELECT *, (embedding <=> %s) AS similarity_score FROM document ORDER BY similarity_score LIMIT %s\", (embedding, num_chunks))\n",
    "        rows = cur.fetchall()\n",
    "        ids = [row[0] for row in rows]\n",
    "        context = [{\"text\": row[1]} for row in rows]\n",
    "        sources = [row[2] for row in rows]\n",
    "        scores = [row[4] for row in rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, item in enumerate(context):\n",
    "    print (ids[i])\n",
    "    print (scores[i])\n",
    "    print (sources[i])\n",
    "    print (item[\"text\"])\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, embedding_model, k):\n",
    "    embedding = np.array(embedding_model.embed_query(query))\n",
    "    with psycopg.connect(os.environ[\"DB_CONNECTION_STRING\"]) as conn:\n",
    "        register_vector(conn)\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(\"SELECT * FROM document ORDER BY embedding <=> %s LIMIT %s\", (embedding, k),)\n",
    "            rows = cur.fetchall()\n",
    "            semantic_context = [{\"id\": row[0], \"text\": row[1], \"source\": row[2]} for row in rows]\n",
    "    return semantic_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag.generate import prepare_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag.utils import get_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(\n",
    "    llm, temperature=0.0, stream=True,\n",
    "    system_content=\"\", assistant_content=\"\", user_content=\"\", \n",
    "    max_retries=1, retry_interval=60):\n",
    "    \"\"\"Generate response from an LLM.\"\"\"\n",
    "    retry_count = 0\n",
    "    client = get_client(llm=llm)\n",
    "    messages = [{\"role\": role, \"content\": content} for role, content in [\n",
    "        (\"system\", system_content), \n",
    "        (\"assistant\", assistant_content), \n",
    "        (\"user\", user_content)] if content]\n",
    "    while retry_count <= max_retries:\n",
    "        try:\n",
    "            chat_completion = client.chat.completions.create(\n",
    "                model=llm,\n",
    "                temperature=temperature,\n",
    "                stream=stream,\n",
    "                messages=messages,\n",
    "            )\n",
    "            return prepare_response(chat_completion, stream=stream)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Exception: {e}\")\n",
    "            time.sleep(retry_interval)  # default is per-minute rate limits\n",
    "            retry_count += 1\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_results = semantic_search(query=query, embedding_model=embedding_model, k=5)\n",
    "context = [item[\"text\"] for item in context_results]\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate response\n",
    "query = \"What are the different kinds of storage, and how do I use them?\"\n",
    "response = generate_response(\n",
    "    llm=\"gpt-4o\",\n",
    "    temperature=0.0,\n",
    "    stream=True,\n",
    "    system_content=\"Answer the query using the context provided. Be succinct.\",\n",
    "    user_content=f\"query: {query}, context: {context}\")\n",
    "# Stream response\n",
    "for content in response:\n",
    "    print(content, end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag.embed import get_embedding_model\n",
    "from rag.utils import get_num_tokens, trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryAgent:\n",
    "    def __init__(self, embedding_model_name=\"text-embedding-3-large\",\n",
    "                 llm=\"gpt-4o\", temperature=0.0, \n",
    "                 max_context_length=4096, system_content=\"\", assistant_content=\"\"):\n",
    "        \n",
    "        # Embedding model\n",
    "        self.embedding_model = OpenAIEmbeddings(\n",
    "            model=\"text-embedding-3-large\",\n",
    "            openai_api_base=os.environ[\"OPENAI_API_BASE\"],\n",
    "            openai_api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    "            )\n",
    "        \n",
    "        # Context length (restrict input length to 50% of total context length)\n",
    "        max_context_length = int(0.5*max_context_length)\n",
    "        \n",
    "        # LLM\n",
    "        self.llm = llm\n",
    "        self.temperature = temperature\n",
    "        self.context_length = max_context_length - get_num_tokens(system_content + assistant_content)\n",
    "        self.system_content = system_content\n",
    "        self.assistant_content = assistant_content\n",
    "\n",
    "    def __call__(self, query, num_chunks=5, stream=True):\n",
    "        # Get sources and context\n",
    "        context_results = semantic_search(\n",
    "            query=query, \n",
    "            embedding_model=self.embedding_model, \n",
    "            k=num_chunks)\n",
    "            \n",
    "        # Generate response\n",
    "        context = [item[\"text\"] for item in context_results]\n",
    "        sources = [item[\"source\"] for item in context_results]\n",
    "        user_content = f\"query: {query}, context: {context}\"\n",
    "        answer = generate_response(\n",
    "            llm=self.llm,\n",
    "            temperature=self.temperature,\n",
    "            stream=stream,\n",
    "            system_content=self.system_content,\n",
    "            assistant_content=self.assistant_content,\n",
    "            user_content=trim(user_content, self.context_length))\n",
    "\n",
    "        # Result\n",
    "        result = {\n",
    "            \"question\": query,\n",
    "            \"sources\": sources,\n",
    "            \"answer\": answer,\n",
    "            \"llm\": self.llm,\n",
    "        }\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_name = \"text-embedding-3-large\"\n",
    "llm = \"gpt-4o\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What happens when I duplicate a workspace?\"\n",
    "system_content = \"Answer the query using the context provided. Be succinct.\"\n",
    "agent = QueryAgent(\n",
    "    embedding_model_name=embedding_model_name,\n",
    "    llm=llm,\n",
    "    max_context_length=MAX_CONTEXT_LENGTHS[llm],\n",
    "    system_content=system_content)\n",
    "result = agent(query=query, stream=False)\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
